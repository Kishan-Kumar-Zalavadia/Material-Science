{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPO/cHcKTx9mdGHOSVeFheB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import json\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","# Load the JSON data\n","with open('allData_result.json', 'r') as file:\n","    data = json.load(file)\n","\n","# Filter out entries without composition\n","filtered_data = [entry for entry in data if entry['composition'] is not None]\n","\n","# Prepare features (X) and target (y)\n","X = []\n","y = []\n","\n","for entry in filtered_data:\n","    features = [entry['b0'], entry['d1'], entry['d2'], entry['ei'], entry['ee']]\n","    X.append(features)\n","\n","    composition = entry['composition']\n","    composition_values = [composition.get(element, 0) for element in set().union(*[d['composition'].keys() for d in filtered_data])]\n","    y.append(composition_values)\n","\n","X = np.array(X)\n","y = np.array(y)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Scale the features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Create and train the MLP model\n","model = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n","model.fit(X_train_scaled, y_train)\n","\n","# Make predictions on the test set\n","y_pred = model.predict(X_test_scaled)\n","\n","# Calculate the Mean Squared Error and R-squared score\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","\n","print(f\"Mean Squared Error: {mse}\")\n","print(f\"R-squared Score: {r2}\")\n","\n","# Calculate element-wise accuracy\n","element_accuracy = 1 - np.mean(np.abs(y_test - y_pred) / np.maximum(y_test, 1e-8), axis=0)\n","print(\"Element-wise Accuracy:\")\n","for i, acc in enumerate(element_accuracy):\n","    print(f\"Element {i+1}: {acc:.4f}\")\n","\n","# Overall accuracy\n","overall_accuracy = np.mean(element_accuracy)\n","print(f\"\\nOverall Accuracy: {overall_accuracy:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qu_sMGb4V2q8","executionInfo":{"status":"ok","timestamp":1729533951843,"user_tz":300,"elapsed":24467,"user":{"displayName":"Kishan kumar","userId":"12369662999305954818"}},"outputId":"6b3b4c73-621f-4e36-abbc-db3bf6819460"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Squared Error: 1544.9264367906496\n","R-squared Score: -38.302451338327494\n","Element-wise Accuracy:\n","Element 1: -653875100.0376\n","Element 2: -204953085.5696\n","Element 3: -3.0223\n","Element 4: -339382809.2256\n","Element 5: -726921699.4998\n","Element 6: -108743286.0658\n","Element 7: -32949485.0103\n","Element 8: -95568410.4502\n","Element 9: -26481368.6371\n","Element 10: -42210140.1532\n","Element 11: -58644731.5821\n","Element 12: -655191758.0805\n","Element 13: -59859077.7481\n","Element 14: -1101310697.2299\n","Element 15: -82158729.9511\n","Element 16: -44250917.8262\n","Element 17: -26584357.4424\n","Element 18: -39071471.4258\n","Element 19: -10.0597\n","Element 20: -51906004.6724\n","Element 21: -100999779.0691\n","Element 22: -191273196.8207\n","Element 23: -227212481.4841\n","Element 24: -73972850.0743\n","Element 25: -165917925.4072\n","Element 26: -730237045.8909\n","Element 27: -37546986.5875\n","Element 28: -740302272.0503\n","Element 29: -5.9010\n","Element 30: -45155904.1585\n","Element 31: -72506796.8854\n","\n","Overall Accuracy: -217264141.5490\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["import json\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.preprocessing import StandardScaler, RobustScaler\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.impute import SimpleImputer\n","\n","# Load the JSON data\n","with open('allData_result.json', 'r') as file:\n","    data = json.load(file)\n","\n","# Filter out entries without composition\n","filtered_data = [entry for entry in data if entry['composition'] is not None]\n","\n","# Prepare features (X) and target (y)\n","X = []\n","y = []\n","\n","for entry in filtered_data:\n","    features = [entry['b0'], entry['d1'], entry['d2'], entry['ei'], entry['ee']]\n","    X.append(features)\n","\n","    composition = entry['composition']\n","    composition_values = [composition.get(element, 0) for element in set().union(*[d['composition'].keys() for d in filtered_data])]\n","    y.append(composition_values)\n","\n","X = np.array(X)\n","y = np.array(y)\n","\n","# Handle infinite and NaN values\n","X = np.nan_to_num(X, nan=0, posinf=0, neginf=0)\n","y = np.nan_to_num(y, nan=0, posinf=0, neginf=0)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Scale the features and target\n","feature_scaler = RobustScaler()\n","X_train_scaled = feature_scaler.fit_transform(X_train)\n","X_test_scaled = feature_scaler.transform(X_test)\n","\n","target_scaler = RobustScaler()\n","y_train_scaled = target_scaler.fit_transform(y_train)\n","y_test_scaled = target_scaler.transform(y_test)\n","\n","# Create and train the MLP model\n","model = MLPRegressor(hidden_layer_sizes=(50, 25), max_iter=5000, alpha=0.01, random_state=42)\n","model.fit(X_train_scaled, y_train_scaled)\n","\n","# Make predictions on the test set\n","y_pred_scaled = model.predict(X_test_scaled)\n","y_pred = target_scaler.inverse_transform(y_pred_scaled)\n","\n","# Calculate the Mean Squared Error and R-squared score\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","\n","print(f\"Mean Squared Error: {mse}\")\n","print(f\"R-squared Score: {r2}\")\n","\n","# Calculate element-wise accuracy\n","element_accuracy = 1 - np.mean(np.abs(y_test - y_pred) / (np.abs(y_test) + 1e-8), axis=0)\n","print(\"Element-wise Accuracy:\")\n","for i, acc in enumerate(element_accuracy):\n","    print(f\"Element {i+1}: {acc:.4f}\")\n","\n","# Overall accuracy\n","overall_accuracy = np.mean(element_accuracy)\n","print(f\"\\nOverall Accuracy: {overall_accuracy:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TRm0GA3qXUX2","executionInfo":{"status":"ok","timestamp":1729533956883,"user_tz":300,"elapsed":5042,"user":{"displayName":"Kishan kumar","userId":"12369662999305954818"}},"outputId":"b30bf6b2-c207-4a51-873d-9c693a91d940"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Squared Error: 75.89587895061625\n","R-squared Score: -0.9624944133629785\n","Element-wise Accuracy:\n","Element 1: -456535956.7056\n","Element 2: -13273518.7344\n","Element 3: -2.8963\n","Element 4: -60282868.3702\n","Element 5: -1596081400.6737\n","Element 6: -21074369.1221\n","Element 7: -15910801.3077\n","Element 8: -18847465.1441\n","Element 9: -20359342.4998\n","Element 10: -21440968.9835\n","Element 11: -26941788.2145\n","Element 12: -432428861.2596\n","Element 13: -24007442.5234\n","Element 14: -404117454.5787\n","Element 15: -19411037.2243\n","Element 16: -9319499.2791\n","Element 17: -17817934.9171\n","Element 18: -9711728.9414\n","Element 19: -4.9176\n","Element 20: -20381881.2515\n","Element 21: -31087165.7582\n","Element 22: -169553117.0720\n","Element 23: -77903927.5932\n","Element 24: -14182117.8009\n","Element 25: -65547592.6010\n","Element 26: -700808068.0795\n","Element 27: -10614725.1815\n","Element 28: -713206711.4518\n","Element 29: -4.4077\n","Element 30: -26709311.7612\n","Element 31: -17464425.3253\n","\n","Overall Accuracy: -161774886.9218\n"]}]},{"cell_type":"code","source":["import json\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_squared_error, r2_score\n","import matplotlib.pyplot as plt\n","\n","# Load the JSON data\n","with open('allData_result.json', 'r') as file:\n","    data = json.load(file)\n","\n","# Filter out entries without composition\n","filtered_data = [entry for entry in data if entry['composition'] is not None]\n","\n","# Prepare features (X) and target (y)\n","X = []\n","y = []\n","composition_keys = set()\n","\n","for entry in filtered_data:\n","    features = [entry['b0'], entry['d1'], entry['d2'], entry['ei'], entry['ee']]\n","    X.append(features)\n","\n","    composition = entry['composition']\n","    composition_keys.update(composition.keys())\n","    y.append(composition)\n","\n","# Convert to DataFrame for easier analysis\n","df = pd.DataFrame(X, columns=['b0', 'd1', 'd2', 'ei', 'ee'])\n","for key in composition_keys:\n","    df[key] = [entry['composition'].get(key, 0) for entry in filtered_data]\n","\n","# Data exploration\n","print(\"Feature statistics:\")\n","print(df[['b0', 'd1', 'd2', 'ei', 'ee']].describe())\n","\n","print(\"\\nComposition statistics:\")\n","print(df[list(composition_keys)].describe())\n","\n","# Correlation analysis\n","correlation = df.corr()\n","plt.figure(figsize=(12, 10))\n","plt.imshow(correlation, cmap='coolwarm', aspect='auto')\n","plt.colorbar()\n","plt.xticks(range(len(correlation.columns)), correlation.columns, rotation=90)\n","plt.yticks(range(len(correlation.columns)), correlation.columns)\n","plt.title(\"Correlation Heatmap\")\n","plt.tight_layout()\n","plt.savefig(\"correlation_heatmap.png\")\n","plt.close()\n","\n","# Prepare data for modeling\n","X = df[['b0', 'd1', 'd2', 'ei', 'ee']].values\n","y = df[list(composition_keys)].values\n","\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Scale features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Train a Random Forest model\n","model = RandomForestRegressor(n_estimators=100, random_state=42)\n","model.fit(X_train_scaled, y_train)\n","\n","# Make predictions\n","y_pred = model.predict(X_test_scaled)\n","\n","# Evaluate the model\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","\n","print(f\"\\nMean Squared Error: {mse}\")\n","print(f\"R-squared Score: {r2}\")\n","\n","# Calculate element-wise accuracy\n","element_accuracy = 1 - np.mean(np.abs(y_test - y_pred) / (np.abs(y_test) + 1e-8), axis=0)\n","print(\"\\nElement-wise Accuracy:\")\n","for i, acc in enumerate(element_accuracy):\n","    print(f\"{list(composition_keys)[i]}: {acc:.4f}\")\n","\n","# Overall accuracy\n","overall_accuracy = np.mean(element_accuracy)\n","print(f\"\\nOverall Accuracy: {overall_accuracy:.4f}\")\n","\n","# Feature importance\n","feature_importance = model.feature_importances_\n","for i, importance in enumerate(['b0', 'd1', 'd2', 'ei', 'ee']):\n","    print(f\"{importance}: {feature_importance[i]:.4f}\")\n","\n","plt.figure(figsize=(10, 6))\n","plt.bar(['b0', 'd1', 'd2', 'ei', 'ee'], feature_importance)\n","plt.title(\"Feature Importance\")\n","plt.tight_layout()\n","plt.savefig(\"feature_importance.png\")\n","plt.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pI6UsVn9XfgS","executionInfo":{"status":"ok","timestamp":1729533970216,"user_tz":300,"elapsed":13336,"user":{"displayName":"Kishan kumar","userId":"12369662999305954818"}},"outputId":"7d93f4b8-e858-4e8e-b523-24f552b1b410"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature statistics:\n","                d1         d2         ei            ee\n","count    23.000000  23.000000  23.000000  2.300000e+01\n","mean   1611.932293   0.252174   0.150000  1.000000e-03\n","std    1630.615123   0.101666   0.058387  6.651416e-19\n","min     190.979233   0.200000   0.100000  1.000000e-03\n","25%     626.734771   0.200000   0.100000  1.000000e-03\n","50%     839.622557   0.200000   0.150000  1.000000e-03\n","75%    2506.137990   0.200000   0.200000  1.000000e-03\n","max    5879.000000   0.450000   0.250000  1.000000e-03\n","\n","Composition statistics:\n","              Nb         Cu         Cr         Si         Ti          B  \\\n","count  23.000000  23.000000  23.000000  23.000000  23.000000  23.000000   \n","mean    1.460870   0.195652   7.443478   0.869565   4.669565   0.086957   \n","std     4.291825   0.516525   7.978541   1.937767  10.551624   0.288104   \n","min     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n","25%     0.000000   0.000000   1.000000   0.000000   0.000000   0.000000   \n","50%     0.000000   0.000000   1.500000   0.000000   0.000000   0.000000   \n","75%     0.600000   0.000000  15.000000   0.000000   2.900000   0.000000   \n","max    20.000000   1.500000  20.000000   5.000000  35.000000   1.000000   \n","\n","               I         Mo          M          A  ...         Mn         Ta  \\\n","count  23.000000  23.000000  23.000000  23.000000  ...  23.000000  23.000000   \n","mean    0.043478   0.117391   0.043478   0.086957  ...   7.217391   0.434783   \n","std     0.208514   0.309915   0.208514   0.288104  ...   9.553469   1.440520   \n","min     0.000000   0.000000   0.000000   0.000000  ...   0.000000   0.000000   \n","25%     0.000000   0.000000   0.000000   0.000000  ...   0.000000   0.000000   \n","50%     0.000000   0.000000   0.000000   0.000000  ...   1.000000   0.000000   \n","75%     0.000000   0.000000   0.000000   0.000000  ...  20.000000   0.000000   \n","max     1.000000   0.900000   1.000000   1.000000  ...  20.000000   5.000000   \n","\n","               G         Al         Hf          J         Zr         Co  \\\n","count  23.000000  23.000000  23.000000  23.000000  23.000000  23.000000   \n","mean    0.043478   1.417391   3.260870   0.043478   3.260870   9.508696   \n","std     0.208514   3.489680   8.707193   0.208514   8.707193   9.499470   \n","min     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n","25%     0.000000   0.000000   0.000000   0.000000   0.000000   1.000000   \n","50%     0.000000   0.000000   0.000000   0.000000   0.000000   1.000000   \n","75%     0.000000   0.000000   0.000000   0.000000   0.000000  20.000000   \n","max     1.000000  10.200000  27.500000   1.000000  27.500000  20.000000   \n","\n","               W          L  \n","count  23.000000  23.000000  \n","mean    0.052174   0.043478  \n","std     0.137740   0.208514  \n","min     0.000000   0.000000  \n","25%     0.000000   0.000000  \n","50%     0.000000   0.000000  \n","75%     0.000000   0.000000  \n","max     0.400000   1.000000  \n","\n","[8 rows x 31 columns]\n","\n","Mean Squared Error: 15341.469595447006\n","R-squared Score: -339.56193583522014\n","\n","Element-wise Accuracy:\n","Nb: -372542856.1429\n","Cu: -5785713.4086\n","Cr: -0.4869\n","Si: -23571427.6929\n","Ti: -680742856.1429\n","B: -9857141.8571\n","I: -3571427.5714\n","Mo: -2442856.1429\n","M: -3571427.5714\n","A: -4571427.5714\n","T: -3571427.5714\n","X: -319999999.0000\n","E: -4571427.5714\n","Ni: -18500557366.0836\n","R: -3571427.5714\n","U: -3571427.5714\n","H: -999999.0000\n","Ww: -20142856.1429\n","Fe: -1.0921\n","N: -3571427.5714\n","C: -34657141.8571\n","Mn: -174999999.8959\n","Ta: -49285713.2857\n","G: -3571427.5714\n","Al: -26228570.6997\n","Hf: -591071427.5714\n","J: -3571427.5714\n","Zr: -591071427.5714\n","Co: -1.2507\n","W: -1085713.2857\n","L: -3571427.5714\n","\n","Overall Accuracy: -691817057.1579\n","b0: 0.0694\n","d1: 0.8226\n","d2: 0.0210\n","ei: 0.0869\n","ee: 0.0000\n"]}]},{"cell_type":"code","source":["import json\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","import matplotlib.pyplot as plt\n","\n","# Load the JSON data\n","with open('allData_result.json', 'r') as file:\n","    data = json.load(file)\n","\n","# Filter out entries without composition\n","filtered_data = [entry for entry in data if entry['composition'] is not None]\n","\n","# Prepare features (X) and target (y)\n","X = []\n","y = []\n","composition_keys = set()\n","\n","for entry in filtered_data:\n","    features = [entry['b0'], entry['d1'], entry['d2'], entry['ei'], entry['ee']]\n","    X.append(features)\n","\n","    composition = entry['composition']\n","    composition_keys.update(composition.keys())\n","    y.append(composition)\n","\n","# Convert to DataFrame\n","df = pd.DataFrame(X, columns=['b0', 'd1', 'd2', 'ei', 'ee'])\n","for key in composition_keys:\n","    df[key] = [1 if entry['composition'].get(key, 0) > 0 else 0 for entry in filtered_data]\n","\n","# Select top 10 most common elements\n","top_elements = df[list(composition_keys)].sum().sort_values(ascending=False).head(10).index.tolist()\n","\n","# Prepare data for modeling\n","X = df[['b0', 'd1', 'd2', 'ei']].values  # Removing 'ee' as it's constant\n","y = df[top_elements].values\n","\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Scale features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Train a Multi-output Random Forest Classifier\n","model = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n","model.fit(X_train_scaled, y_train)\n","\n","# Make predictions\n","y_pred = model.predict(X_test_scaled)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test.flatten(), y_pred.flatten())\n","print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n","\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred, target_names=top_elements))\n","\n","# Feature importance\n","feature_importance = np.mean([estimator.feature_importances_ for estimator in model.estimators_], axis=0)\n","for i, importance in enumerate(['b0', 'd1', 'd2', 'ei']):\n","    print(f\"{importance}: {feature_importance[i]:.4f}\")\n","\n","plt.figure(figsize=(10, 6))\n","plt.bar(['b0', 'd1', 'd2', 'ei'], feature_importance)\n","plt.title(\"Feature Importance\")\n","plt.tight_layout()\n","plt.savefig(\"feature_importance_classification.png\")\n","plt.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C01xTiBJXrjg","executionInfo":{"status":"ok","timestamp":1729533983837,"user_tz":300,"elapsed":13624,"user":{"displayName":"Kishan kumar","userId":"12369662999305954818"}},"outputId":"26dfca77-370b-4317-de84-e51a00d4ae9c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Overall Accuracy: 0.7800\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","          Co       1.00      0.80      0.89         5\n","          Cr       1.00      0.80      0.89         5\n","          Fe       1.00      0.80      0.89         5\n","          Ni       1.00      0.80      0.89         5\n","          Mn       0.50      0.67      0.57         3\n","           C       0.00      0.00      0.00         0\n","          Nb       0.00      0.00      0.00         0\n","          Ti       0.00      0.00      0.00         0\n","          Al       0.00      0.00      0.00         2\n","          Si       0.00      0.00      0.00         0\n","\n","   micro avg       0.82      0.72      0.77        25\n","   macro avg       0.45      0.39      0.41        25\n","weighted avg       0.86      0.72      0.78        25\n"," samples avg       0.72      0.72      0.72        25\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["b0: 0.1107\n","d1: 0.5951\n","d2: 0.0884\n","ei: 0.2058\n"]}]},{"cell_type":"markdown","source":["# Train the model"],"metadata":{"id":"EN4VwAYSixmo"}},{"cell_type":"code","source":["import json\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","import matplotlib.pyplot as plt\n","\n","# Load the JSON data\n","with open('allData_result.json', 'r') as file:\n","    data = json.load(file)\n","\n","# Filter out entries without composition\n","filtered_data = [entry for entry in data if entry['composition'] is not None]\n","\n","# Prepare features (X) and target (y)\n","X = []\n","y = []\n","composition_keys = set()\n","\n","for entry in filtered_data:\n","    features = [entry['b0'], entry['d1'], entry['d2'], entry['ei'], entry['ee']]\n","    X.append(features)\n","\n","    composition = entry['composition']\n","    composition_keys.update(composition.keys())\n","    y.append(composition)\n","\n","# Convert to DataFrame\n","df = pd.DataFrame(X, columns=['b0', 'd1', 'd2', 'ei', 'ee'])\n","for key in composition_keys:\n","    df[key] = [1 if entry['composition'].get(key, 0) > 0 else 0 for entry in filtered_data]\n","\n","# Select top 10 most common elements\n","top_elements = df[list(composition_keys)].sum().sort_values(ascending=False).head(10).index.tolist()\n","\n","# Prepare data for modeling\n","X = df[['b0', 'd1', 'd2', 'ei']].values  # Removing 'ee' as it's constant\n","y = df[top_elements].values\n","\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Scale features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Train a Multi-output Random Forest Classifier\n","model = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n","model.fit(X_train_scaled, y_train)\n","\n","# Make predictions\n","y_pred = model.predict(X_test_scaled)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test.flatten(), y_pred.flatten())\n","print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n","\n","# Handle undefined metrics by setting zero_division to 0\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred, target_names=top_elements, zero_division=0))\n","\n","# Analyze data distribution in training and test sets\n","train_counts = np.sum(y_train, axis=0)\n","test_counts = np.sum(y_test, axis=0)\n","element_distribution = pd.DataFrame({'Element': top_elements, 'Train Count': train_counts, 'Test Count': test_counts})\n","print(\"\\nElement Distribution:\")\n","print(element_distribution)\n","\n","# Feature importance\n","feature_importance = np.mean([estimator.feature_importances_ for estimator in model.estimators_], axis=0)\n","for i, importance in enumerate(['b0', 'd1', 'd2', 'ei']):\n","    print(f\"{importance}: {feature_importance[i]:.4f}\")\n","\n","# Plot feature importance\n","plt.figure(figsize=(10, 6))\n","plt.bar(['b0', 'd1', 'd2', 'ei'], feature_importance)\n","plt.title(\"Feature Importance\")\n","plt.tight_layout()\n","plt.savefig(\"feature_importance_classification.png\")\n","plt.close()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"smOSK0rThJGR","executionInfo":{"status":"ok","timestamp":1729533995627,"user_tz":300,"elapsed":11793,"user":{"displayName":"Kishan kumar","userId":"12369662999305954818"}},"outputId":"53b737bc-b184-4f28-d14f-625d7e4f74b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Overall Accuracy: 0.7800\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","          Co       1.00      0.80      0.89         5\n","          Cr       1.00      0.80      0.89         5\n","          Fe       1.00      0.80      0.89         5\n","          Ni       1.00      0.80      0.89         5\n","          Mn       0.50      0.67      0.57         3\n","           C       0.00      0.00      0.00         0\n","          Nb       0.00      0.00      0.00         0\n","          Ti       0.00      0.00      0.00         0\n","          Al       0.00      0.00      0.00         2\n","          Si       0.00      0.00      0.00         0\n","\n","   micro avg       0.82      0.72      0.77        25\n","   macro avg       0.45      0.39      0.41        25\n","weighted avg       0.86      0.72      0.78        25\n"," samples avg       0.72      0.72      0.72        25\n","\n","\n","Element Distribution:\n","  Element  Train Count  Test Count\n","0      Co           14           5\n","1      Cr           14           5\n","2      Fe           14           5\n","3      Ni           10           5\n","4      Mn           11           3\n","5       C            7           0\n","6      Nb            6           0\n","7      Ti            6           0\n","8      Al            3           2\n","9      Si            4           0\n","b0: 0.1107\n","d1: 0.5951\n","d2: 0.0884\n","ei: 0.2058\n"]}]},{"cell_type":"markdown","source":["# Guess the element"],"metadata":{"id":"QXBMTe9Iisu1"}},{"cell_type":"code","source":["import json\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","import matplotlib.pyplot as plt\n","\n","# Load the JSON data\n","def load_data(file_path):\n","    with open(file_path, 'r') as file:\n","        return json.load(file)\n","\n","# Prepare features (X) and target (y)\n","def prepare_data(data):\n","    X = []\n","    y = []\n","    composition_keys = set()\n","    for entry in data:\n","        if entry['composition'] is not None:\n","            features = [entry['b0'], entry['d1'], entry['d2'], entry['ei']]  # Removed 'ee' as it's constant\n","            X.append(features)\n","            composition = entry['composition']\n","            composition_keys.update(composition.keys())\n","            y.append(composition)\n","    return X, y, list(composition_keys)\n","\n","# Convert data to DataFrame\n","def create_dataframe(X, y, composition_keys, feature_names):\n","    df = pd.DataFrame(X, columns=feature_names)\n","    for key in composition_keys:\n","        df[key] = [1 if entry.get(key, 0) > 0 else 0 for entry in y]\n","    return df\n","\n","# Select top N most common elements\n","def select_top_elements(df, composition_keys, n=10):\n","    return df[list(composition_keys)].sum().sort_values(ascending=False).head(n).index.tolist()\n","\n","# Train the model\n","def train_model(X, y):\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","    scaler = StandardScaler()\n","    X_train_scaled = scaler.fit_transform(X_train)\n","    X_test_scaled = scaler.transform(X_test)\n","    model = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n","    model.fit(X_train_scaled, y_train)\n","    return model, scaler, X_test_scaled, y_test\n","\n","# Evaluate the model\n","def evaluate_model(model, X_test_scaled, y_test, top_elements):\n","    y_pred = model.predict(X_test_scaled)\n","    accuracy = accuracy_score(y_test.flatten(), y_pred.flatten())\n","    print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_test, y_pred, target_names=top_elements, zero_division=0))\n","\n","# Predict compositions for objects without them\n","def predict_compositions(model, scaler, data, feature_names, top_elements):\n","    for entry in data:\n","        if entry['composition'] is None:\n","            features = [[entry[feature] for feature in feature_names]]\n","            features_scaled = scaler.transform(features)\n","            predicted_composition = model.predict(features_scaled)[0]\n","            entry['composition'] = {element: int(pred) for element, pred in zip(top_elements, predicted_composition)}\n","    return data\n","\n","# Save the updated data to a new JSON file\n","def save_data(data, output_file):\n","    with open(output_file, 'w') as file:\n","        json.dump(data, file, indent=2)\n","\n","# Main function\n","def main():\n","    input_file = 'allData_result.json'\n","    output_file = 'allData_result_with_predictions.json'\n","    feature_names = ['b0', 'd1', 'd2', 'ei']\n","\n","    data = load_data(input_file)\n","    X, y, composition_keys = prepare_data(data)\n","    df = create_dataframe(X, y, composition_keys, feature_names)\n","    top_elements = select_top_elements(df, composition_keys)\n","\n","    X = df[feature_names].values\n","    y = df[top_elements].values\n","\n","    model, scaler, X_test_scaled, y_test = train_model(X, y)\n","    evaluate_model(model, X_test_scaled, y_test, top_elements)\n","\n","    updated_data = predict_compositions(model, scaler, data, feature_names, top_elements)\n","    save_data(updated_data, output_file)\n","\n","    print(f\"\\nProcessing complete. Updated data saved to {output_file}\")\n","\n","    # Feature importance\n","    feature_importance = np.mean([estimator.feature_importances_ for estimator in model.estimators_], axis=0)\n","    for i, importance in enumerate(feature_names):\n","        print(f\"{importance}: {feature_importance[i]:.4f}\")\n","\n","    # Plot feature importance\n","    plt.figure(figsize=(10, 6))\n","    plt.bar(feature_names, feature_importance)\n","    plt.title(\"Feature Importance\")\n","    plt.tight_layout()\n","    plt.savefig(\"feature_importance_classification.png\")\n","    plt.close()\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ufQN4Mdhud_","executionInfo":{"status":"ok","timestamp":1729534008535,"user_tz":300,"elapsed":12910,"user":{"displayName":"Kishan kumar","userId":"12369662999305954818"}},"outputId":"bd31eed8-2286-4016-a4cd-fbc1c2b23932"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Overall Accuracy: 0.7800\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","          Co       1.00      0.80      0.89         5\n","          Cr       1.00      0.80      0.89         5\n","          Fe       1.00      0.80      0.89         5\n","          Ni       1.00      0.80      0.89         5\n","          Mn       0.50      0.67      0.57         3\n","           C       0.00      0.00      0.00         0\n","          Nb       0.00      0.00      0.00         0\n","          Ti       0.00      0.00      0.00         0\n","          Al       0.00      0.00      0.00         2\n","          Si       0.00      0.00      0.00         0\n","\n","   micro avg       0.82      0.72      0.77        25\n","   macro avg       0.45      0.39      0.41        25\n","weighted avg       0.86      0.72      0.78        25\n"," samples avg       0.72      0.72      0.72        25\n","\n","\n","Processing complete. Updated data saved to allData_result_with_predictions.json\n","b0: 0.1107\n","d1: 0.5951\n","d2: 0.0884\n","ei: 0.2058\n"]}]},{"cell_type":"markdown","source":["# Periodic table elements."],"metadata":{"id":"X50zDJm0ineY"}},{"cell_type":"code","source":["import json\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","import matplotlib.pyplot as plt\n","\n","# List of elements in the periodic table\n","PERIODIC_TABLE_ELEMENTS = [\n","    'H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne', 'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'Ar',\n","    'K', 'Ca', 'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn', 'Ga', 'Ge', 'As', 'Se', 'Br', 'Kr',\n","    'Rb', 'Sr', 'Y', 'Zr', 'Nb', 'Mo', 'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd', 'In', 'Sn', 'Sb', 'Te', 'I', 'Xe',\n","    'Cs', 'Ba', 'La', 'Ce', 'Pr', 'Nd', 'Pm', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Ho', 'Er', 'Tm', 'Yb', 'Lu',\n","    'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', 'Pt', 'Au', 'Hg', 'Tl', 'Pb', 'Bi', 'Po', 'At', 'Rn',\n","    'Fr', 'Ra', 'Ac', 'Th', 'Pa', 'U', 'Np', 'Pu', 'Am', 'Cm', 'Bk', 'Cf', 'Es', 'Fm', 'Md', 'No', 'Lr',\n","    'Rf', 'Db', 'Sg', 'Bh', 'Hs', 'Mt', 'Ds', 'Rg', 'Cn', 'Nh', 'Fl', 'Mc', 'Lv', 'Ts', 'Og'\n","]\n","\n","# Load the JSON data\n","def load_data(file_path):\n","    with open(file_path, 'r') as file:\n","        return json.load(file)\n","\n","# Prepare features (X) and target (y)\n","def prepare_data(data):\n","    X = []\n","    y = []\n","    composition_keys = set()\n","    for entry in data:\n","        if entry['composition'] is not None:\n","            features = [entry['b0'], entry['d1'], entry['d2'], entry['ei']]  # Removed 'ee' as it's constant\n","            X.append(features)\n","            composition = {k: v for k, v in entry['composition'].items() if k in PERIODIC_TABLE_ELEMENTS}\n","            composition_keys.update(composition.keys())\n","            y.append(composition)\n","    return X, y, list(composition_keys)\n","\n","# Convert data to DataFrame\n","def create_dataframe(X, y, composition_keys, feature_names):\n","    df = pd.DataFrame(X, columns=feature_names)\n","    for key in composition_keys:\n","        df[key] = [1 if entry.get(key, 0) > 0 else 0 for entry in y]\n","    return df\n","\n","# Select top N most common elements\n","def select_top_elements(df, composition_keys, n=15):\n","    return df[list(composition_keys)].sum().sort_values(ascending=False).head(n).index.tolist()\n","\n","# Train the model\n","def train_model(X, y):\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","    scaler = StandardScaler()\n","    X_train_scaled = scaler.fit_transform(X_train)\n","    X_test_scaled = scaler.transform(X_test)\n","    model = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n","    model.fit(X_train_scaled, y_train)\n","    return model, scaler, X_test_scaled, y_test\n","\n","# Evaluate the model\n","def evaluate_model(model, X_test_scaled, y_test, top_elements):\n","    y_pred = model.predict(X_test_scaled)\n","    accuracy = accuracy_score(y_test.flatten(), y_pred.flatten())\n","    print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_test, y_pred, target_names=top_elements, zero_division=0))\n","\n","# Predict compositions for objects without them\n","def predict_compositions(model, scaler, data, feature_names, top_elements):\n","    for entry in data:\n","        if entry['composition'] is None:\n","            features = [[entry[feature] for feature in feature_names]]\n","            features_scaled = scaler.transform(features)\n","            predicted_composition = model.predict(features_scaled)[0]\n","            entry['composition'] = {\n","                element: int(pred) for element, pred in zip(top_elements, predicted_composition)\n","                if element in PERIODIC_TABLE_ELEMENTS and pred > 0\n","            }\n","    return data\n","\n","# Save the updated data to a new JSON file\n","def save_data(data, output_file):\n","    with open(output_file, 'w') as file:\n","        json.dump(data, file, indent=2)\n","\n","# Main function\n","def main():\n","    input_file = 'allData_result.json'\n","    output_file = 'allData_result_with_predictions.json'\n","    feature_names = ['b0', 'd1', 'd2', 'ei']\n","\n","    data = load_data(input_file)\n","    X, y, composition_keys = prepare_data(data)\n","    df = create_dataframe(X, y, composition_keys, feature_names)\n","    top_elements = select_top_elements(df, composition_keys)\n","\n","    X = df[feature_names].values\n","    y = df[top_elements].values\n","\n","    model, scaler, X_test_scaled, y_test = train_model(X, y)\n","    evaluate_model(model, X_test_scaled, y_test, top_elements)\n","\n","    updated_data = predict_compositions(model, scaler, data, feature_names, top_elements)\n","    save_data(updated_data, output_file)\n","\n","    print(f\"\\nProcessing complete. Updated data saved to {output_file}\")\n","\n","    # Feature importance\n","    feature_importance = np.mean([estimator.feature_importances_ for estimator in model.estimators_], axis=0)\n","    for i, importance in enumerate(feature_names):\n","        print(f\"{importance}: {feature_importance[i]:.4f}\")\n","\n","    # Plot feature importance\n","    plt.figure(figsize=(10, 6))\n","    plt.bar(feature_names, feature_importance)\n","    plt.title(\"Feature Importance\")\n","    plt.tight_layout()\n","    plt.savefig(\"feature_importance_classification.png\")\n","    plt.close()\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BXa5OjhEilDS","executionInfo":{"status":"ok","timestamp":1729624625292,"user_tz":300,"elapsed":3841,"user":{"displayName":"Kishan kumar","userId":"12369662999305954818"}},"outputId":"cef0e13c-13e1-45a4-d371-3aab1f79d41c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Overall Accuracy: 0.7067\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","          Co       1.00      0.60      0.75         5\n","          Cr       1.00      0.60      0.75         5\n","          Fe       1.00      0.60      0.75         5\n","          Mn       0.33      0.33      0.33         3\n","          Ni       1.00      0.60      0.75         5\n","           C       0.00      0.00      0.00         0\n","          Ti       0.00      0.00      0.00         0\n","          Nb       0.00      0.00      0.00         0\n","          Si       0.00      0.00      0.00         0\n","          Zr       0.00      0.00      0.00         0\n","          Al       0.00      0.00      0.00         2\n","          Cu       0.00      0.00      0.00         0\n","          Hf       0.00      0.00      0.00         0\n","          Ta       0.00      0.00      0.00         0\n","           B       0.00      0.00      0.00         0\n","\n","   micro avg       0.57      0.52      0.54        25\n","   macro avg       0.29      0.18      0.22        25\n","weighted avg       0.84      0.52      0.64        25\n"," samples avg       0.52      0.52      0.52        25\n","\n","\n","Processing complete. Updated data saved to allData_result_with_predictions.json\n","b0: 0.0776\n","d1: 0.5663\n","d2: 0.1098\n","ei: 0.2463\n"]}]}]}